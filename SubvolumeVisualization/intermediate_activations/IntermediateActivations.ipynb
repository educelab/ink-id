{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Intermediate Activatinos\n",
    "Based on https://github.com/himanshurawlani/convnet-interpretability-keras/blob/master/Visualizing%20intermediate%20activations/visualizing_intermediate_activations.ipynb\n",
    "\n",
    "https://towardsdatascience.com/visual-interpretability-for-convolutional-neural-networks-2453856210ce\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Config file (dataset.info)\n",
    "[input_data]  \n",
    "dataset_name = CarbonPhantom  \n",
    "subvol_type = NearestNeighbor  \n",
    "col = Col6  \n",
    "ground_truth = CarbonInk  \n",
    "dataset_num = 1  \n",
    "path_prefix = /home/mhaya2/3d-utilities/SubvolumeVisualization/Data/labeled_subvolume_sampler/  \n",
    "\n",
    "[output]  \n",
    "output_prefix = /home/mhaya2/3d-utilities/SubvolumeVisualization/Results/IntermediateActivations2/  \n",
    "\n",
    "[saved_weights]  \n",
    "weight_path = /home/mhaya2/3d-utilities/SubvolumeVisualization/SavedWeights/  \n",
    "weight_file = m1252444_200000.pt  \n",
    "\n",
    "[hook_layers]  \n",
    "conv1 = y  \n",
    "batch_norm1 = y  \n",
    "conv2 = y  \n",
    "batch_norm2 = y  \n",
    "conv3 = y  \n",
    "batch_norm3 = y  \n",
    "conv4 = y  \n",
    "batch_norm4 = y  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First things first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from the dataset info file\n",
    "import configparser\n",
    "data_info = configparser.ConfigParser()\n",
    "data_info.read('dataset.info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input data\n",
    "dataset_name =  data_info['input_data']['dataset_name']\n",
    "subvol_type = data_info['input_data']['subvol_type']\n",
    "col = data_info['input_data']['col']\n",
    "ground_truth = data_info['input_data']['ground_truth']\n",
    "dataset_num = data_info['input_data']['dataset_num']\n",
    "\n",
    "# Define output data\n",
    "output_prefix = data_info['output']['output_prefix']\n",
    "\n",
    "# Define weights\n",
    "weight_path = data_info['saved_weights']['weight_path']\n",
    "weight_file = data_info['saved_weights']['weight_file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "print(\"dataset name: \", dataset_name)\n",
    "print(\"subvol_type: \", subvol_type)\n",
    "print(\"col: \", col)\n",
    "print(\"ground_truth: \", ground_truth)\n",
    "print(\"dataset_num: \", dataset_num)\n",
    "print(\"output_prefix: \", output_prefix)\n",
    "print(\"weight_path: \", weight_path)\n",
    "print(\"weight_file: \", weight_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log file\n",
    "metadata = {} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output dir\n",
    "output_subdir = f\"{dataset_name}/{subvol_type}/{col}/{ground_truth}/{dataset_num}/{weight_file[:8]}\"\n",
    "\n",
    "output_dir = output_prefix + output_subdir\n",
    "\n",
    "metadata['output_dir'] = output_dir\n",
    "print(\"output_dir: \", output_dir)\n",
    "\n",
    "import os\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Volume Rendering Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def render_3d(cube, output_file, unit=None):\n",
    "  X, Y, Z = np.mgrid[0:cube.shape[0], 0:cube.shape[1], 0:cube.shape[2]]\n",
    "  vol = go.Volume(\n",
    "      name=dataset_name,\n",
    "      x = X.flatten(),\n",
    "      y = Y.flatten(),\n",
    "      z = Z.flatten(),\n",
    "      value = cube.flatten(),\n",
    "      opacity = 0.3,\n",
    "      opacityscale = 0.3,\n",
    "      surface_count = 10,\n",
    "      colorscale='rainbow',\n",
    "      slices_z = dict(show=True, locations=[10]),\n",
    "    )\n",
    "  fig = go.Figure(data=vol)\n",
    "\n",
    "  if unit:\n",
    "    vals = []\n",
    "    texts = []\n",
    "    for i in range (0,cube.shape[0],8):\n",
    "      vals.append(i)\n",
    "      texts.append(str(i*unit))\n",
    "\n",
    "    fig.update_layout(scene = dict(\n",
    "                    xaxis = dict(\n",
    "                        ticktext=texts,\n",
    "                        tickvals=vals),\n",
    "                    yaxis = dict(\n",
    "                        ticktext=texts,\n",
    "                        tickvals=vals),\n",
    "                    zaxis = dict(\n",
    "                        ticktext=texts,\n",
    "                        tickvals=vals)))\n",
    "    \n",
    "  fig.write_image(output_file)\n",
    "  fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name is a directory that contains a series of .tif files\n",
    "path_prefix = \"/home/mhaya2/3d-utilities/SubvolumeVisualization/Data/labeled_subvolume_sampler/\"\n",
    "data_path = f\"{dataset_name}/{subvol_type}/{col}/{ground_truth}Individual/{dataset_num}\"\n",
    "dataset = Path(f'{path_prefix}/{data_path}/')\n",
    "files = list(dataset.glob('*.tif'))\n",
    "files.sort(key=lambda f: int(re.sub(r'[^0-9]*', \"\", str(f))))\n",
    "\n",
    "metadata['dataset'] = str(dataset)\n",
    "\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subvolume = []\n",
    "images = []\n",
    "for f in files:\n",
    "  i = Image.open(f)\n",
    "  subvolume.append(np.array(Image.open(f), dtype=np.float32))\n",
    "  images.append(i)\n",
    "\n",
    "# convert to numpy\n",
    "subvolume = np.array(subvolume) \n",
    "print(np.shape(subvolume))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the Input Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_renderings = f'{output_dir}/InputImages'\n",
    "\n",
    "if not os.path.exists(input_renderings):\n",
    "    os.makedirs(input_renderings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See the original TIFF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_ROWS = 8\n",
    "IMGs_IN_ROW = 6\n",
    "\n",
    "f, ax_arr = plt.subplots(NUM_ROWS, IMGs_IN_ROW, figsize=(18,24))\n",
    "for j, row in enumerate(ax_arr):\n",
    "    for i, ax in enumerate(row):\n",
    "        ax.imshow(images[j*IMGs_IN_ROW+i])\n",
    "        ax.set_title(f'image {j*IMGs_IN_ROW+i}')\n",
    "\n",
    "title = output_subdir\n",
    "f.suptitle(title, fontsize=16)\n",
    "plt.savefig(f'{input_renderings}/tiff_slices.png')\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See slices in all 3 directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ROWS = 8\n",
    "IMGs_IN_ROW = 6\n",
    "\n",
    "f, ax_arr = plt.subplots(NUM_ROWS*3, IMGs_IN_ROW, figsize=(18,60))\n",
    "for j, row in enumerate(ax_arr):\n",
    "    if j < 8:\n",
    "      for i, ax in enumerate(row):\n",
    "        ax.imshow(subvolume[j*IMGs_IN_ROW+i, :, :])\n",
    "        ax.set_title(f'x-slice {j*IMGs_IN_ROW+i}')\n",
    "    elif j < 16:\n",
    "      for i, ax in enumerate(row):\n",
    "        ax.imshow(subvolume[:,(j-8)*IMGs_IN_ROW+i, :])\n",
    "        ax.set_title(f'y-slice {(j-8)*IMGs_IN_ROW+i}')\n",
    "    else:\n",
    "      for i, ax in enumerate(row):\n",
    "        ax.imshow(subvolume[:,:,(j-16)*IMGs_IN_ROW+i])\n",
    "        ax.set_title(f'z-slice {(j-16)*IMGs_IN_ROW+i}') \n",
    "\n",
    "title = output_subdir\n",
    "f.suptitle(title, fontsize=16)\n",
    "plt.savefig(f'{input_renderings}/slices.png')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render Subvolume 3D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_3d(subvolume, f'{input_renderings}/plotly.png', unit=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Inkid CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Taken from model.py\n",
    "\n",
    "def conv_output_shape(input_shape, kernel_size: Union[int, tuple], stride: Union[int, tuple],\n",
    "                      padding: Union[int, tuple], dilation: Union[int, tuple] = 1):\n",
    "    dim = len(input_shape)\n",
    "    # Accept either ints or tuples for these parameters. If int, then convert into tuple (same value all for all dims).\n",
    "    if isinstance(kernel_size, int):\n",
    "        kernel_size = (kernel_size,) * dim\n",
    "    if isinstance(stride, int):\n",
    "        stride = (stride,) * dim\n",
    "    if isinstance(padding, int):\n",
    "        padding = (padding,) * dim\n",
    "    if isinstance(dilation, int):\n",
    "        dilation = (dilation,) * dim\n",
    "    # https://pytorch.org/docs/stable/nn.html#torch.nn.Conv3d See \"Shape:\" section.\n",
    "    return tuple(math.floor((input_shape[d] + 2 * padding[d] - dilation[d] * (kernel_size[d] - 1) - 1) / stride[d] + 1)\n",
    "                 for d in range(dim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Slightly modified version of what is in model.py\n",
    "\n",
    "class Test3DCNN(torch.nn.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "\n",
    "    super().__init__()\n",
    "\n",
    "    input_shape = [48, 48, 48] # to match the subvolume size we are testing\n",
    "    in_channels = 1\n",
    "    batch_norm_momentum = 0.9\n",
    "\n",
    "    self.relu = torch.nn.ReLU()\n",
    "    self._in_channels = in_channels\n",
    "    self._batch_norm = True\n",
    "\n",
    "    filters = filters = [32, 16, 8, 4]\n",
    "    paddings = [1, 1, 1, 1]\n",
    "    kernel_sizes = [3, 3, 3, 3]\n",
    "    strides = [1, 2, 2, 2]\n",
    "\n",
    "    self.conv1 = torch.nn.Conv3d(in_channels=in_channels, out_channels=filters[0],\n",
    "                                     kernel_size=kernel_sizes[0], stride=strides[0], padding=paddings[0])\n",
    "    torch.nn.init.xavier_uniform_(self.conv1.weight)\n",
    "    torch.nn.init.zeros_(self.conv1.bias)\n",
    "    self.batch_norm1 = torch.nn.BatchNorm3d(num_features=filters[0], momentum=batch_norm_momentum)\n",
    "    shape = conv_output_shape(input_shape, kernel_sizes[0], strides[0], paddings[0])\n",
    "\n",
    "    self.conv2 = torch.nn.Conv3d(in_channels=filters[0], out_channels=filters[1],\n",
    "                                  kernel_size=kernel_sizes[1], stride=strides[1], padding=paddings[1])\n",
    "    torch.nn.init.xavier_uniform_(self.conv2.weight)\n",
    "    torch.nn.init.zeros_(self.conv2.bias)\n",
    "    self.batch_norm2 = torch.nn.BatchNorm3d(num_features=filters[1], momentum=batch_norm_momentum)\n",
    "    shape = conv_output_shape(shape, kernel_sizes[1], strides[1], paddings[1])\n",
    "\n",
    "    self.conv3 = torch.nn.Conv3d(in_channels=filters[1], out_channels=filters[2],\n",
    "                                  kernel_size=kernel_sizes[2], stride=strides[2], padding=paddings[2])\n",
    "    torch.nn.init.xavier_uniform_(self.conv3.weight)\n",
    "    torch.nn.init.zeros_(self.conv3.bias)\n",
    "    self.batch_norm3 = torch.nn.BatchNorm3d(num_features=filters[2], momentum=batch_norm_momentum)\n",
    "    shape = conv_output_shape(shape, kernel_sizes[2], strides[2], paddings[2])\n",
    "\n",
    "    self.conv4 = torch.nn.Conv3d(in_channels=filters[2], out_channels=filters[3],\n",
    "                                  kernel_size=kernel_sizes[3], stride=strides[3], padding=paddings[3])\n",
    "    torch.nn.init.xavier_uniform_(self.conv4.weight)\n",
    "    torch.nn.init.zeros_(self.conv4.bias)\n",
    "    self.batch_norm4 = torch.nn.BatchNorm3d(num_features=filters[3], momentum=batch_norm_momentum)\n",
    "    shape = conv_output_shape(shape, kernel_sizes[3], strides[3], paddings[3])\n",
    "    self.output_shape = (filters[3],) + shape\n",
    "\n",
    "  def forward(self, x):\n",
    "      if self._in_channels > 1:\n",
    "          x = torch.squeeze(x)\n",
    "      #y = self.conv1(x)\n",
    "      #y = self.relu(y)\n",
    "      #if self._batch_norm:\n",
    "      #    y = self.batch_norm1(y)\n",
    "      #y = self.conv2(y)\n",
    "      #y = self.relu(y)\n",
    "      #if self._batch_norm:\n",
    "      #    y = self.batch_norm2(y)\n",
    "      #y = self.conv3(y)\n",
    "      #y = self.relu(y)\n",
    "      #if self._batch_norm:\n",
    "      #    y = self.batch_norm3(y)\n",
    "      #y = self.conv4(y)\n",
    "      #y = self.relu(y)\n",
    "      #if self._batch_norm:\n",
    "      #    y = self.batch_norm4(y)\n",
    "\n",
    "      y = self.relu(self.conv1(x))\n",
    "      if self._batch_norm:\n",
    "        y = self.batch_norm1(y)  \n",
    "      y = self.relu(self.conv2(y))\n",
    "      if self._batch_norm:\n",
    "        y = self.batch_norm2(y) \n",
    "      y = self.relu(self.conv3(y))\n",
    "      if self._batch_norm:\n",
    "        y = self.batch_norm3(y) \n",
    "      y = self.relu(self.conv4(y))\n",
    "      if self._batch_norm:\n",
    "        y = self.batch_norm4(y)          \n",
    "      return y\n",
    "\n",
    "class LinearInkDecoder(torch.nn.Module):\n",
    "    def __init__(self, drop_rate, input_shape, output_neurons):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc = torch.nn.Linear(int(np.prod(input_shape)), output_neurons)\n",
    "        self.dropout = torch.nn.Dropout(p=drop_rate)\n",
    "\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.flatten(x)\n",
    "        y = self.fc(y)\n",
    "        y = self.dropout(y)\n",
    "        # Add some dimensions to match the dimensionality of label which is always 2D even if shape is (1, 1)\n",
    "        y = torch.unsqueeze(y, 2)\n",
    "        y = torch.unsqueeze(y, 3)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model(model):\n",
    "  for param_tensor in model.state_dict():\n",
    "      print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put Models Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Test3DCNN()\n",
    "decoder = LinearInkDecoder(0.6, encoder.output_shape, 2)\n",
    "model = torch.nn.Sequential(encoder, decoder)\n",
    "\n",
    "metadata['model'] = str(model)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#weight_path = '/home/mhaya2/3d-utilities/SubvolumeVisualization/SavedWeights/'\n",
    "#weight_file = 'm1252444_200000.pt'\n",
    "model.load_state_dict(torch.load(weight_path+weight_file, map_location=torch.device('cpu'))['model_state_dict'], strict=False)\n",
    "\n",
    "metadata['saved_weights'] = weight_file\n",
    "# set to eval mode\n",
    "model.eval()\n",
    "\n",
    "# Sanity check\n",
    "print_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hook function for recording activations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = {}\n",
    "\n",
    "def get_activation(name):\n",
    "  # hook signature\n",
    "  def hook_function(model, input, output):\n",
    "    activation[name] = output.detach()\n",
    "  return hook_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create desired hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_info['hook_layers']['conv1'] == 'y':\n",
    "    h1c_name = 'conv1'\n",
    "    h1c = model[0].conv1.register_forward_hook(get_activation(h1c_name))\n",
    "\n",
    "if data_info['hook_layers']['batch_norm1'] == 'y':\n",
    "    h1b_name = 'batch_norm1'\n",
    "    h1b = model[0].batch_norm1.register_forward_hook(get_activation(h1b_name))\n",
    "\n",
    "if data_info['hook_layers']['conv2'] == 'y':  \n",
    "    h2c_name = 'conv2'\n",
    "    h2c = model[0].conv2.register_forward_hook(get_activation(h2c_name))\n",
    "    \n",
    "if data_info['hook_layers']['batch_norm2'] == 'y':\n",
    "    h2b_name = 'batch_norm2'\n",
    "    h2b = model[0].batch_norm2.register_forward_hook(get_activation(h2b_name))\n",
    "\n",
    "if data_info['hook_layers']['conv3'] == 'y':\n",
    "    h3c_name = 'conv3'\n",
    "    h3c = model[0].conv3.register_forward_hook(get_activation(h3c_name))\n",
    "\n",
    "if data_info['hook_layers']['batch_norm3'] == 'y':\n",
    "    h3b_name = 'batch_norm3'\n",
    "    h3b = model[0].conv3.register_forward_hook(get_activation(h3b_name))\n",
    "\n",
    "if data_info['hook_layers']['conv4'] == 'y':   \n",
    "    h4c_name = 'conv4'\n",
    "    h4c = model[0].conv4.register_forward_hook(get_activation(h4c_name))\n",
    "    \n",
    "if data_info['hook_layers']['batch_norm4'] == 'y':\n",
    "    h4b_name = 'batch_norm4'\n",
    "    h4b = model[0].batch_norm4.register_forward_hook(get_activation(h4b_name))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subvolume = np.array(subvolume)  # Converting a list to a numpy array\n",
    "\n",
    "# Add two extra axes\n",
    "subvolume=subvolume[np.newaxis, np.newaxis, ...]\n",
    "print(\"final subvolume input shape:\", subvolume.shape)\n",
    "\n",
    "# Input subvolume\n",
    "subvolume = torch.from_numpy(subvolume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Model Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=model(subvolume)\n",
    "\n",
    "metadata['prediction_output'] = str(output)\n",
    "\n",
    "print(output)\n",
    "\n",
    "prediction = output.argmax(dim=1).item()\n",
    "metadata['prediction'] = str(prediction)\n",
    "#print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(activation)\n",
    "#print(activation[layer].shape)\n",
    "\n",
    "h1c.remove()\n",
    "h1b.remove()\n",
    "h2c.remove()\n",
    "h2b.remove()\n",
    "h3c.remove()\n",
    "h3b.remove()\n",
    "h4c.remove()\n",
    "h4b.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#activations = activation[layer].numpy()\n",
    "#activations.shape\n",
    "\n",
    "conv1_activations = activation[h1c_name].numpy()\n",
    "print(\"conv1_activations: \", conv1_activations.shape)\n",
    "bnorm1_activations = activation[h1b_name].numpy()\n",
    "print(\"bnorm1_activations: \", bnorm1_activations.shape)\n",
    "\n",
    "conv2_activations = activation[h2c_name].numpy()\n",
    "print(\"conv2_activations: \", conv2_activations.shape)\n",
    "bnorm2_activations = activation[h2b_name].numpy()\n",
    "print(\"bnorm2_activations: \", bnorm2_activations.shape)\n",
    "\n",
    "conv3_activations = activation[h3c_name].numpy()\n",
    "print(\"conv3_activations: \", conv3_activations.shape)\n",
    "bnorm3_activations = activation[h3b_name].numpy()\n",
    "print(\"bnorm3_activations: \", bnorm3_activations.shape)\n",
    "\n",
    "conv4_activations = activation[h4c_name].numpy()\n",
    "print(\"conv4_activations: \", conv4_activations.shape)\n",
    "bnorm4_activations = activation[h4b_name].numpy()\n",
    "print(\"bnorm4_activations: \", bnorm4_activations.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the Activations on Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_path = f'{output_dir}/IntermediateActivations'\n",
    "\n",
    "if not os.path.exists(activations_path):\n",
    "    os.makedirs(activations_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import os\n",
    "\n",
    "\n",
    "def plot3d_layer_activations(activations, name, colorscale='viridis', dirname=None):\n",
    "  # activations.shape = (1, n_filter, cube[0], cube[1],  cube[2],  )\n",
    "  # Try height 300 for each row\n",
    "  # width 1200\n",
    "\n",
    "    #TODO: Add the dataset name etc.\n",
    "    dpath = dirname + '/' + name\n",
    "    \n",
    "    if not os.path.exists(dpath):\n",
    "        os.makedirs(dpath)\n",
    "    min_val = np.min(activations)\n",
    "    max_val = np.max(activations)\n",
    "    \n",
    "    print(\"layer: \", name)\n",
    "    print(\"Max value: \", max_val)\n",
    "    print(\"Min value: \", min_val)\n",
    "    \n",
    "    metadata[name] = {\"max\":str(max_val), \"min\": str(min_val)}\n",
    "    n_filters = activations.shape[1]  \n",
    "\n",
    "    for i in range(n_filters):\n",
    "        signal = activations[0,i,:,:,:] # resulting shape is 3 dimentions.\n",
    "\n",
    "        X, Y, Z = np.mgrid[0:signal.shape[0], 0:signal.shape[1], 0:signal.shape[2]]\n",
    "\n",
    "        fig = go.Figure(data=go.Volume(\n",
    "            name=dataset_name,\n",
    "            x = X.flatten(),\n",
    "            y = Y.flatten(),\n",
    "            z = Z.flatten(),\n",
    "            value = signal.flatten(),\n",
    "            cmin = min_val,\n",
    "            cmax = max_val,\n",
    "            opacity = 0.3,\n",
    "            opacityscale = 0.3,\n",
    "            surface_count = 8,\n",
    "            colorscale=colorscale,\n",
    "            #slices_z = dict(show=True, locations=[10]),\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(scene = dict(\n",
    "                    xaxis = dict(showticklabels=False),\n",
    "                    yaxis = dict(showticklabels=False),\n",
    "                    zaxis = dict(showticklabels=False)))\n",
    "    \n",
    "        filename = name + '_' + str(i) + '.png'\n",
    "        print (\"writing in : \", dpath + '/' + filename)\n",
    "        \n",
    "        fig.write_image(dpath + '/' + filename)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot3d_layer_activations(conv1_activations, h1c_name + \"_activations\", colorscale='viridis', dirname = activations_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot3d_layer_activations(conv2_activations, h2c_name + \"_activations\", colorscale='viridis', dirname = activations_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot3d_layer_activations(conv3_activations, h3c_name + \"_activations\", colorscale='viridis', dirname = activations_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot3d_layer_activations(conv4_activations, h4c_name + \"_activations\", colorscale='viridis', dirname = activations_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot3d_layer_activations(bnorm1_activations, h1b_name + \"_activations\", colorscale='viridis', dirname = activations_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot3d_layer_activations(bnorm2_activations, h2b_name + \"_activations\", colorscale='viridis', dirname = activations_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot3d_layer_activations(bnorm3_activations, h3b_name + \"_activations\", colorscale='viridis', dirname = activations_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot3d_layer_activations(bnorm4_activations, h4b_name + \"_activations\", colorscale='viridis', dirname = activations_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last layers slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv4_activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_slices(layer_activations, name, output_dir):\n",
    "    n_filters = layer_activations.shape[1]\n",
    "    \n",
    "    for n in range(n_filters):\n",
    "        filter_data = layer_activations[0,n,:,:,:]\n",
    "        \n",
    "        NUM_ROWS = 1\n",
    "        IMGs_IN_ROW = layer_activations.shape[2]\n",
    "\n",
    "        f, ax_arr = plt.subplots(NUM_ROWS*3, IMGs_IN_ROW, figsize=(18,12))\n",
    "        for j, row in enumerate(ax_arr):\n",
    "            if j == 0:\n",
    "              for i, ax in enumerate(row):\n",
    "                ax.imshow(filter_data[j*IMGs_IN_ROW+i, :, :])\n",
    "                ax.set_title(f'x-slices {j*IMGs_IN_ROW+i}')\n",
    "            elif j == 1:\n",
    "              for i, ax in enumerate(row):\n",
    "                ax.imshow(filter_data[:,(j-1)*IMGs_IN_ROW+i, :])\n",
    "                ax.set_title(f'y-slices {(j-1)*IMGs_IN_ROW+i}')\n",
    "            else:\n",
    "              for i, ax in enumerate(row):\n",
    "                ax.imshow(filter_data[:,:,(j-2)*IMGs_IN_ROW+i])\n",
    "                ax.set_title(f'z-slices {(j-2)*IMGs_IN_ROW+i}') \n",
    "\n",
    "\n",
    "        slices_title = f'{title}_{name}_filter{str(n)}'\n",
    "        f.suptitle(slices_title, fontsize=16)\n",
    "        \n",
    "        plt.savefig(f'{output_dir}/{name}/filter_{str(n)}_slices.png')\n",
    "        plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_slices(conv4_activations, h4c_name + '_activations', activations_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_slices(bnorm4_activations, h4b_name + '_activations', activations_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{output_dir}/metadata.json\", \"w\") as outfile: \n",
    "    json.dump(metadata, outfile, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
