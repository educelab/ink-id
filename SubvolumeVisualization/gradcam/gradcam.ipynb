{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradcam (Jupyter Notebook)\n",
    "\n",
    "### Prerequisites\n",
    "- gradcam_input_data.info file with the following:\n",
    "    TODO:\n",
    "- model.py file \n",
    "- saved weights (.pt file)\n",
    "- input subvolume organized as *.tif images or 3D numpy arrray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from the dataset info file\n",
    "import configparser\n",
    "data_info = configparser.ConfigParser()\n",
    "data_info.read('gradcam_input_data.info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input data\n",
    "input_prefix = data_info['input_data']['input_prefix']\n",
    "data_dir = data_info['input_data']['data_dir']\n",
    "\n",
    "# Define model\n",
    "weights = data_info['model']['saved_weights']\n",
    "\n",
    "# Define output data\n",
    "output_prefix = data_info['output_data']['output_prefix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Sanity check\n",
    "print(\"input_prefix: \", input_prefix)\n",
    "print(\"data_dir: \", data_dir)\n",
    "print(\"weights: \", weights)\n",
    "print(\"output_prefix: \", output_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load input subvolume data (can be skipped if starting with a numpy 3D array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = Path(f'{input_prefix}/{data_dir}/')\n",
    "\n",
    "# print(dataset_dir)\n",
    "files = list(dataset_dir.glob('*.tif'))\n",
    "files.sort(key=lambda f: int(re.sub(r'[^0-9]*', \"\", str(f))))\n",
    "\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and convert to a numpy 3D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subvolume = []\n",
    "images = []\n",
    "for f in files:\n",
    "  i = Image.open(f)\n",
    "  subvolume.append(np.array(Image.open(f), dtype=np.float32))\n",
    "  images.append(i)\n",
    "\n",
    "# convert to numpy\n",
    "subvolume = np.array(subvolume) \n",
    "print(np.shape(subvolume))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradCam Wrapper class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gradcam3D:\n",
    "\n",
    "  def __init__(self, encoder, decoder):\n",
    "    \n",
    "    # TODO: Can we pass the model (torch.nnSequential) and the layers for hooks?\n",
    "\n",
    "    # Create a model\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "    self.model=torch.nn.Sequential(encoder, decoder)\n",
    "\n",
    "    # placeholder for the gradients\n",
    "    self.gradients = None\n",
    "\n",
    "    # placeholder for the activations\n",
    "    self.activations = None\n",
    "\n",
    "    # Register hooks\n",
    "    self.model[0].conv4.register_backward_hook(self.printgradnorm)\n",
    "    self.model[0].conv4.register_forward_hook(self.printnorm)\n",
    "\n",
    "  def printnorm(self, module, input, output):\n",
    "    print('input size:', input[0].size(), '    <---- activations')\n",
    "    self.activations = input[0].detach()     \n",
    "\n",
    "  def printgradnorm(self, module, grad_input, grad_output):\n",
    "\n",
    "    print('grad_input size:', grad_input[0].size(), '    <---- gradients' )\n",
    "    self.gradients = grad_input[0].detach()\n",
    "\n",
    "  def print_model(self):\n",
    "    for param_tensor in self.model.state_dict():\n",
    "      print(param_tensor, \"\\t\", self.model.state_dict()[param_tensor].size())\n",
    "\n",
    "  def load_weights(self, path):\n",
    "      self.model.load_state_dict(torch.load(path, map_location=torch.device('cpu'))['model_state_dict'], strict=False)\n",
    "      self.model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the saved weights (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "checkpoint = torch.load(weights, map_location=torch.device('cpu'))\n",
    "print(checkpoint.keys()) # --> dict_keys(['epoch', 'batch', 'model_state_dict', 'optimizer_state_dict'])\n",
    "print(checkpoint['model_state_dict'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load encoder and decoder to the Gradcam wrapper class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_localcopy as saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subvolume_shape = [48, 48, 48]\n",
    "batch_norm_momentum = 0.9\n",
    "no_batch_norm = False\n",
    "filters = [32, 16, 8, 4] \n",
    "in_channels = 1\n",
    "\n",
    "encoder = saved_model.Subvolume3DcnnEncoder(\n",
    "                subvolume_shape=subvolume_shape,\n",
    "                batch_norm_momentum=batch_norm_momentum, \n",
    "                no_batch_norm=no_batch_norm, \n",
    "                filters=filters, \n",
    "                in_channels=in_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = saved_model.LinearInkDecoder(\n",
    "                drop_rate=0.5, \n",
    "                input_shape=encoder.output_shape,\n",
    "                output_neurons=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inkid_gradcam = Gradcam3D(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inkid_gradcam.print_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the saved weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inkid_gradcam.load_weights(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the input subvolume by adding 2 extra axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subvolume=subvolume[np.newaxis, np.newaxis, ...]\n",
    "print(\"final subvolume input shape:\", subvolume.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert subvolume to Torch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subvolume = torch.from_numpy(subvolume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push the subvolume through and obtain prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=inkid_gradcam.model(subvolume).argmax(dim=1).item()\n",
    "print(\"prediction:\", prediction)   # 1 is ink; 0 is no ink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate activation \"hotness\" (GradCam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inkid_gradcam.model(subvolume)[:, prediction, :, :].backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = inkid_gradcam.activations\n",
    "print(\"activations size: \", activations.size())\n",
    "gradients = inkid_gradcam.gradients\n",
    "print(\"gradients size: \", gradients.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expression (1) in https://arxiv.org/pdf/1610.02391.pdf\n",
    "\\begin{equation}\n",
    "\\alpha^c_k = 1/Z\\sum_{i}\\sum_{j} \\frac{\\partial y^c}{\\partial A^k_{ij}}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_gradients = torch.mean(gradients, dim=[0, 2, 3, 4])\n",
    "# Sanity Check\n",
    "print(pooled_gradients.size())  # ---> [8]\n",
    "print(pooled_gradients[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expression (2) in https://arxiv.org/pdf/1610.02391.pdf\n",
    "\\begin{equation}\n",
    "L^c_{Grad-CAM} = ReLU(\\sum_{k}\\alpha^c_K A^k) \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight the channels by corresponding gradients\n",
    "for i in range(8):\n",
    "    activations[:, i, :, :, :] *= pooled_gradients[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average the channels of the activations\n",
    "heatmap = torch.mean(activations, dim=1).squeeze()\n",
    "#heatmap.size() # torch.Size([9, 9, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu on top of the heatmap (we are not insterested in negative values)\n",
    "heatmap = np.maximum(heatmap.detach(), 0)\n",
    "heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the heatmap\n",
    "heatmap /= torch.max(heatmap)\n",
    "heatmap[0][11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "X, Y, Z = np.mgrid[0:12:, 0:12, 0:12]\n",
    "values = heatmap\n",
    "\n",
    "gradient_map = go.Figure(data=go.Volume(\n",
    "    x=X.flatten(),\n",
    "    y=Y.flatten(),\n",
    "    z=Z.flatten(),\n",
    "    value=values.flatten(),\n",
    "    isomin=0.1,\n",
    "    isomax=1.0,\n",
    "    opacity=0.2, # needs to be small to see through all surfaces\n",
    "    surface_count=10, # needs to be a large number for good volume rendering\n",
    "    colorscale = 'rainbow'\n",
    "    ))\n",
    "\n",
    "gradient_map.update_layout(showlegend=False)\n",
    "\n",
    "#gradient_map.write_image(\"images/prediction_gradient_map.png\")\n",
    "gradient_map.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Subvolume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subvolume_orig = torch.squeeze(subvolume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, Z = np.mgrid[0:48, 0:48, 0:48]\n",
    "# TODO: Change this to accommodate more shapes\n",
    "\n",
    "subvolume_map = go.Figure(data=go.Volume(\n",
    "    x=X.flatten(),\n",
    "    y=Y.flatten(),\n",
    "    z=Z.flatten(),\n",
    "    value=subvolume_orig.flatten(),\n",
    "    isomin=0.1,\n",
    "    isomax=torch.max(subvolume_orig).item()*0.9,\n",
    "    opacity=0.3, # needs to be small to see through all surfaces\n",
    "    surface_count=10, # needs to be a large number for good volume rendering\n",
    "    colorscale = 'Greys'\n",
    "    ))\n",
    "\n",
    "subvolume_map.update_layout(showlegend=False)\n",
    "#subvolume_map.write_image(\"images/subvolume_map.png\")\n",
    "subvolume_map.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If overlapping is desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import cv2\n",
    "gradient_img = cv2.imread('images/gradient_map.png')\n",
    "subvolume_img = cv2.imread('images/subvolume_map.png')\n",
    "\n",
    "superimposed_img = cv2.addWeighted(gradient_img, 0.35, subvolume_img, 0.65, 0)\n",
    "cv2.imwrite('images/superimposed.png', superimposed_img)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
